{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":302713,"sourceType":"datasetVersion","datasetId":125828}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\nuser_secrets.set_tensorflow_credential(user_credential)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T23:40:32.414951Z","iopub.execute_input":"2024-11-06T23:40:32.415414Z","iopub.status.idle":"2024-11-06T23:40:32.609032Z","shell.execute_reply.started":"2024-11-06T23:40:32.415370Z","shell.execute_reply":"2024-11-06T23:40:32.607882Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -q datasets\n!pip install -q evaluate\n!pip install -q jiwer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-06T23:40:32.611014Z","iopub.execute_input":"2024-11-06T23:40:32.611317Z","iopub.status.idle":"2024-11-06T23:41:10.059282Z","shell.execute_reply.started":"2024-11-06T23:40:32.611284Z","shell.execute_reply":"2024-11-06T23:41:10.057966Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"path_to_dataset = '/kaggle/input/medical-speech-transcription-and-intent/Medical Speech, Transcription, and Intent'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T23:41:10.060879Z","iopub.execute_input":"2024-11-06T23:41:10.061247Z","iopub.status.idle":"2024-11-06T23:41:10.067599Z","shell.execute_reply.started":"2024-11-06T23:41:10.061208Z","shell.execute_reply":"2024-11-06T23:41:10.066557Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(path_to_dataset + \"/overview-of-recordings.csv\")\ndf.sample(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T23:41:10.068925Z","iopub.execute_input":"2024-11-06T23:41:10.069501Z","iopub.status.idle":"2024-11-06T23:41:10.544500Z","shell.execute_reply.started":"2024-11-06T23:41:10.069466Z","shell.execute_reply":"2024-11-06T23:41:10.543466Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"     audio_clipping  audio_clipping:confidence background_noise_audible  \\\n3826    no_clipping                        1.0                 no_noise   \n4701    no_clipping                        1.0                 no_noise   \n2714    no_clipping                        1.0              light_noise   \n\n      background_noise_audible:confidence  overall_quality_of_the_audio  \\\n3826                               0.6826                          3.33   \n4701                               1.0000                          3.67   \n2714                               1.0000                          4.00   \n\n        quiet_speaker  quiet_speaker:confidence  speaker_id  \\\n3826  audible_speaker                       1.0    43856216   \n4701  audible_speaker                       1.0    43620482   \n2714  audible_speaker                       1.0    38202325   \n\n                                          file_download  \\\n3826  https://ml.sandbox.cf3.us/cgi-bin/index.cgi?do...   \n4701  https://ml.sandbox.cf3.us/cgi-bin/index.cgi?do...   \n2714  https://ml.sandbox.cf3.us/cgi-bin/index.cgi?do...   \n\n                          file_name  \\\n3826  1249120_43856216_87443859.wav   \n4701  1249120_43620482_36193265.wav   \n2714  1249120_38202325_40303876.wav   \n\n                                                 phrase         prompt  \\\n3826                          It itches inside my ears.       Ear ache   \n4701  Chronic disease of hair follicles and sebaceou...           Acne   \n2714        I feel pain inside I do not know what it is  Internal pain   \n\n      writer_id  \n3826    1883056  \n4701   44218005  \n2714   43730599  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audio_clipping</th>\n      <th>audio_clipping:confidence</th>\n      <th>background_noise_audible</th>\n      <th>background_noise_audible:confidence</th>\n      <th>overall_quality_of_the_audio</th>\n      <th>quiet_speaker</th>\n      <th>quiet_speaker:confidence</th>\n      <th>speaker_id</th>\n      <th>file_download</th>\n      <th>file_name</th>\n      <th>phrase</th>\n      <th>prompt</th>\n      <th>writer_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3826</th>\n      <td>no_clipping</td>\n      <td>1.0</td>\n      <td>no_noise</td>\n      <td>0.6826</td>\n      <td>3.33</td>\n      <td>audible_speaker</td>\n      <td>1.0</td>\n      <td>43856216</td>\n      <td>https://ml.sandbox.cf3.us/cgi-bin/index.cgi?do...</td>\n      <td>1249120_43856216_87443859.wav</td>\n      <td>It itches inside my ears.</td>\n      <td>Ear ache</td>\n      <td>1883056</td>\n    </tr>\n    <tr>\n      <th>4701</th>\n      <td>no_clipping</td>\n      <td>1.0</td>\n      <td>no_noise</td>\n      <td>1.0000</td>\n      <td>3.67</td>\n      <td>audible_speaker</td>\n      <td>1.0</td>\n      <td>43620482</td>\n      <td>https://ml.sandbox.cf3.us/cgi-bin/index.cgi?do...</td>\n      <td>1249120_43620482_36193265.wav</td>\n      <td>Chronic disease of hair follicles and sebaceou...</td>\n      <td>Acne</td>\n      <td>44218005</td>\n    </tr>\n    <tr>\n      <th>2714</th>\n      <td>no_clipping</td>\n      <td>1.0</td>\n      <td>light_noise</td>\n      <td>1.0000</td>\n      <td>4.00</td>\n      <td>audible_speaker</td>\n      <td>1.0</td>\n      <td>38202325</td>\n      <td>https://ml.sandbox.cf3.us/cgi-bin/index.cgi?do...</td>\n      <td>1249120_38202325_40303876.wav</td>\n      <td>I feel pain inside I do not know what it is</td>\n      <td>Internal pain</td>\n      <td>43730599</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Define paths\nbase_dir = path_to_dataset\ncsv_path = os.path.join(base_dir, \"overview-of-recordings.csv\")\nrecordings_dir = os.path.join(base_dir, \"recordings\")\n\n# Debug: Print paths and verify existence\nprint(\"CSV path:\", csv_path)\nprint(\"CSV exists:\", os.path.exists(csv_path))\nprint(\"Recordings directory:\", recordings_dir)\nprint(\"Recordings dir exists:\", os.path.exists(recordings_dir))\n\n# Load and clean CSV data\ntry:\n    # Read CSV with all columns as strings initially\n    df = pd.read_csv(csv_path, dtype=str)\n\n    # Clean column names - remove any spaces and special characters\n    df.columns = df.columns.str.strip().str.lower()\n\n    # Extract relevant columns\n    required_columns = ['file_name', 'phrase', 'prompt', 'writer_id']\n    data = df[required_columns].copy()\n\n    print(\"\\nInitial data shape:\", data.shape)\n    print(\"\\nSample of initial data:\")\n    print(data.head())\n\n    # Add full file paths for each split\n    def get_file_path(filename):\n        # Check each subdirectory for the file\n        for split in ['train', 'test', 'validate']:\n            path = os.path.join(recordings_dir, split, filename)\n            if os.path.exists(path):\n                return path\n        return None\n\n    # Add file paths and filter for existing files\n    data['file_path'] = data['file_name'].apply(get_file_path)\n    data = data.dropna(subset=['file_path'])\n\n    print(\"\\nData after filtering for existing files:\", len(data))\n\n    if not data.empty:\n        # Split the data according to the existing directory structure\n        def get_split(file_path):\n            if 'train' in file_path:\n                return 'train'\n            elif 'test' in file_path:\n                return 'test'\n            else:\n                return 'validate'\n\n        data['split'] = data['file_path'].apply(get_split)\n\n        # Create datasets for each split\n        train_data = data[data['split'] == 'train']\n        test_data = data[data['split'] == 'test']\n        validate_data = data[data['split'] == 'validate']\n\n        print(\"\\nSplit sizes:\")\n        print(f\"Train: {len(train_data)}\")\n        print(f\"Test: {len(test_data)}\")\n        print(f\"Validate: {len(validate_data)}\")\n\n        # Convert to Hugging Face Datasets\n        train_dataset = Dataset.from_pandas(train_data)\n        test_dataset = Dataset.from_pandas(test_data)\n        validate_dataset = Dataset.from_pandas(validate_data)\n\n        # Function to safely show random elements\n        def show_random_elements(dataset, num_examples=5):\n            if len(dataset) == 0:\n                print(\"Dataset is empty\")\n                return\n\n            max_examples = min(num_examples, len(dataset))\n            if max_examples > 0:\n                # Convert to list to avoid numpy int64 issues\n                indices = list(np.random.choice(len(dataset), max_examples, replace=False))\n                print(f\"\\nShowing {max_examples} random examples:\")\n                for i, idx in enumerate(indices):\n                    print(f\"\\nExample {i + 1}:\")\n                    example = dataset[int(idx)]  # Convert index to int\n                    print(f\"Phrase: {example['phrase']}\")\n                    print(f\"Prompt: {example['prompt']}\")\n                    print(f\"File: {os.path.basename(example['file_path'])}\")\n\n        # Show samples from each split\n        print(\"\\nSamples from train dataset:\")\n        show_random_elements(train_dataset)\n\n        print(\"\\nSamples from validation dataset:\")\n        show_random_elements(validate_dataset)\n\n        print(\"\\nSamples from test dataset:\")\n        show_random_elements(test_dataset)\n\n    else:\n        print(\"No valid files found after filtering.\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {str(e)}\")\n    print(\"Please check if the CSV format matches the expected structure.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T23:41:10.547017Z","iopub.execute_input":"2024-11-06T23:41:10.547383Z","iopub.status.idle":"2024-11-06T23:41:19.050497Z","shell.execute_reply.started":"2024-11-06T23:41:10.547348Z","shell.execute_reply":"2024-11-06T23:41:19.049572Z"}},"outputs":[{"name":"stdout","text":"CSV path: /kaggle/input/medical-speech-transcription-and-intent/Medical Speech, Transcription, and Intent/overview-of-recordings.csv\nCSV exists: True\nRecordings directory: /kaggle/input/medical-speech-transcription-and-intent/Medical Speech, Transcription, and Intent/recordings\nRecordings dir exists: True\n\nInitial data shape: (6661, 4)\n\nSample of initial data:\n                       file_name  \\\n0  1249120_43453425_58166571.wav   \n1  1249120_43719934_43347848.wav   \n2  1249120_43719934_53187202.wav   \n3  1249120_31349958_55816195.wav   \n4  1249120_43719934_82524191.wav   \n\n                                              phrase            prompt  \\\n0                    When I remember her I feel down    Emotional pain   \n1  When I carry heavy things I feel like breaking...  Hair falling out   \n2          there is too much pain when i move my arm       Heart hurts   \n3  My son had his lip pierced and it is swollen a...    Infected wound   \n4             My muscles in my lower back are aching    Infected wound   \n\n  writer_id  \n0  21665495  \n1  44088126  \n2  44292353  \n3  43755034  \n4  21665495  \n\nData after filtering for existing files: 6661\n\nSplit sizes:\nTrain: 381\nTest: 5895\nValidate: 385\n\nSamples from train dataset:\n\nShowing 5 random examples:\n\nExample 1:\nPhrase: Sometimes I feel like a claw on my chest that leaves me breathless.\nPrompt: Hard to breath\nFile: 1249120_44246595_13494572.wav\n\nExample 2:\nPhrase: My stomach feels full and upset  and bloating after big meals.\nPrompt: Stomach ache\nFile: 1249120_44197979_99361949.wav\n\nExample 3:\nPhrase: I cut myself and I'm bleeding.\nPrompt: Open wound\nFile: 1249120_44197979_10146538.wav\n\nExample 4:\nPhrase: I have a sharp pain in my abdomen.\nPrompt: Internal pain\nFile: 1249120_44235678_87414469.wav\n\nExample 5:\nPhrase: i was diagnosed with pneumonia, i can't breath easily.\nPrompt: Hard to breath\nFile: 1249120_44176037_58635902.wav\n\nSamples from validation dataset:\n\nShowing 5 random examples:\n\nExample 1:\nPhrase: my child has cough all night, she can't sleep\nPrompt: Cough\nFile: 1249120_44273314_28827187.wav\n\nExample 2:\nPhrase: I never had any acne problem until my last pregnancy, when all of a sudden my back got covered in zits.\nPrompt: Acne\nFile: 1249120_44323331_77964928.wav\n\nExample 3:\nPhrase: I've had this cough for two weeks.\nPrompt: Cough\nFile: 1249120_44259428_67815432.wav\n\nExample 4:\nPhrase: I had internal pain and gases when I ate indian spicy food yesterday\nPrompt: Internal pain\nFile: 1249120_44263136_10880734.wav\n\nExample 5:\nPhrase: There is an injured person\nPrompt: Infected wound\nFile: 1249120_44323331_86516132.wav\n\nSamples from test dataset:\n\nShowing 5 random examples:\n\nExample 1:\nPhrase: red flushes accompanied with itchy\nPrompt: Skin issue\nFile: 1249120_44101988_91618639.wav\n\nExample 2:\nPhrase: I have a problem in the expiration because i have abronchial asthma\nPrompt: Hard to breath\nFile: 1249120_18172663_85283527.wav\n\nExample 3:\nPhrase: There is pain in my joints. I can not bear pain\nPrompt: Joint pain\nFile: 1249120_44091626_85354887.wav\n\nExample 4:\nPhrase: I have pain in my chest that saddens me.\nPrompt: Heart hurts\nFile: 1249120_39142299_83621046.wav\n\nExample 5:\nPhrase: I have some pain when i'm walking around my knees\nPrompt: Joint pain\nFile: 1249120_39154234_17408376.wav\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"data.sample(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T23:41:19.051562Z","iopub.execute_input":"2024-11-06T23:41:19.051853Z","iopub.status.idle":"2024-11-06T23:41:19.064322Z","shell.execute_reply.started":"2024-11-06T23:41:19.051822Z","shell.execute_reply":"2024-11-06T23:41:19.063344Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                           file_name  \\\n897   1249120_41087148_106385661.wav   \n5125   1249120_18172663_76557062.wav   \n2127   1249120_43604449_33808272.wav   \n6307   1249120_40419625_54798514.wav   \n3752  1249120_43612961_108585632.wav   \n\n                                                 phrase          prompt  \\\n897   I've tried reading books, but nothing can chee...  Emotional pain   \n5125                I have this strange rash on my arm.      Skin issue   \n2127         After an hard working day I have foot ache       Foot ache   \n6307                    I have a throbbing in my joints      Joint pain   \n3752                   i can't breath because of  Cough           Cough   \n\n     writer_id                                          file_path split  \n897   44292353  /kaggle/input/medical-speech-transcription-and...  test  \n5125  44140394  /kaggle/input/medical-speech-transcription-and...  test  \n2127  38687371  /kaggle/input/medical-speech-transcription-and...  test  \n6307  44164300  /kaggle/input/medical-speech-transcription-and...  test  \n3752  44124309  /kaggle/input/medical-speech-transcription-and...  test  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>phrase</th>\n      <th>prompt</th>\n      <th>writer_id</th>\n      <th>file_path</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>897</th>\n      <td>1249120_41087148_106385661.wav</td>\n      <td>I've tried reading books, but nothing can chee...</td>\n      <td>Emotional pain</td>\n      <td>44292353</td>\n      <td>/kaggle/input/medical-speech-transcription-and...</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>5125</th>\n      <td>1249120_18172663_76557062.wav</td>\n      <td>I have this strange rash on my arm.</td>\n      <td>Skin issue</td>\n      <td>44140394</td>\n      <td>/kaggle/input/medical-speech-transcription-and...</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>2127</th>\n      <td>1249120_43604449_33808272.wav</td>\n      <td>After an hard working day I have foot ache</td>\n      <td>Foot ache</td>\n      <td>38687371</td>\n      <td>/kaggle/input/medical-speech-transcription-and...</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>6307</th>\n      <td>1249120_40419625_54798514.wav</td>\n      <td>I have a throbbing in my joints</td>\n      <td>Joint pain</td>\n      <td>44164300</td>\n      <td>/kaggle/input/medical-speech-transcription-and...</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>3752</th>\n      <td>1249120_43612961_108585632.wav</td>\n      <td>i can't breath because of  Cough</td>\n      <td>Cough</td>\n      <td>44124309</td>\n      <td>/kaggle/input/medical-speech-transcription-and...</td>\n      <td>test</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"data = data.drop(columns=['prompt', 'writer_id', 'file_name'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T23:41:19.065628Z","iopub.execute_input":"2024-11-06T23:41:19.065963Z","iopub.status.idle":"2024-11-06T23:41:19.074913Z","shell.execute_reply.started":"2024-11-06T23:41:19.065930Z","shell.execute_reply":"2024-11-06T23:41:19.073824Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"data = data.sample(200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T23:41:19.075911Z","iopub.execute_input":"2024-11-06T23:41:19.076266Z","iopub.status.idle":"2024-11-06T23:41:19.086112Z","shell.execute_reply.started":"2024-11-06T23:41:19.076232Z","shell.execute_reply":"2024-11-06T23:41:19.085392Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torchaudio\nimport numpy as np\nimport pandas as pd\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom datasets import Dataset\nimport evaluate\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nimport wandb\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\n# Initialize wandb for experiment tracking\nwandb.init(project=\"medical-speech-recognition\", name=\"whisper-medical-finetuning\")\n\n# Load metric\nwer_metric = evaluate.load(\"wer\")\ncer_metric = evaluate.load(\"cer\")\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # Split inputs and labels since they have to be of different lengths and need different padding methods\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # Replace padding with -100 for loss calculation\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        batch[\"labels\"] = labels\n\n        return batch\n\ndef prepare_dataset(data):\n    \"\"\"Prepare dataset for training\"\"\"\n    \n    def load_audio(example):\n        audio_input, sample_rate = torchaudio.load(example[\"file_path\"])\n        \n        # Convert to mono if needed\n        if audio_input.shape[0] > 1:\n            audio_input = audio_input.mean(dim=0, keepdim=True)\n            \n        # Resample if needed\n        if sample_rate != 16000:\n            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n            audio_input = resampler(audio_input)\n            \n        example[\"input_features\"] = processor(\n            audio_input.squeeze(0),\n            sampling_rate=16000,\n            return_tensors=\"pt\"\n        )[\"input_features\"].squeeze(0)\n        \n        example[\"labels\"] = processor.tokenizer(example[\"phrase\"])[\"input_ids\"]\n        return example\n\n    dataset = Dataset.from_pandas(data)\n    processed_dataset = dataset.map(load_audio, remove_columns=dataset.column_names)\n    return processed_dataset\n\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    # Handle the case where pred_ids is logits\n    if isinstance(pred_ids, tuple):\n        pred_ids = pred_ids[0]\n\n    # Get the most likely token ids if we have logits\n    if len(pred_ids.shape) == 3:\n        pred_ids = np.argmax(pred_ids, axis=-1)\n\n    # Prepare the arrays for decoding\n    pred_ids = [ids for ids in pred_ids]\n    label_ids = [ids for ids in label_ids]\n\n    # Replace -100 with pad token id in labels\n    cleaned_label_ids = []\n    for labels in label_ids:\n        clean_labels = [processor.tokenizer.pad_token_id if label == -100 else label for label in labels]\n        cleaned_label_ids.append(clean_labels)\n\n    # Decode to texts\n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = processor.batch_decode(cleaned_label_ids, skip_special_tokens=True)\n\n    # Compute metrics\n    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"wer\": wer, \"cer\": cer}\n\n# Load model and processor\nmodel_name = \"openai/whisper-tiny\"\nprocessor = WhisperProcessor.from_pretrained(model_name)\nmodel = WhisperForConditionalGeneration.from_pretrained(model_name)\n\n# Prepare datasets\ntrain_data, eval_data = train_test_split(data, test_size=0.2, random_state=42)\ntrain_dataset = prepare_dataset(train_data)\neval_dataset = prepare_dataset(eval_data)\n\n# Initialize data collator\ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-medical-finetuned\",\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=2,\n    learning_rate=1e-5,\n    warmup_steps=10,\n    max_steps=400,\n    gradient_checkpointing=True,\n    fp16=True,\n    evaluation_strategy=\"steps\",\n    eval_steps=10,\n    save_steps=40,\n    logging_steps=10,\n    report_to=\"wandb\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n)\n\n# Initialize Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\nprint(\"Starting training...\")\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T23:41:19.087517Z","iopub.execute_input":"2024-11-06T23:41:19.087895Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msajidala09\u001b[0m (\u001b[33msajidala09-jamia-millia-islamia\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241106_234127-t72y4r4s</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sajidala09-jamia-millia-islamia/medical-speech-recognition/runs/t72y4r4s' target=\"_blank\">whisper-medical-finetuning</a></strong> to <a href='https://wandb.ai/sajidala09-jamia-millia-islamia/medical-speech-recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sajidala09-jamia-millia-islamia/medical-speech-recognition' target=\"_blank\">https://wandb.ai/sajidala09-jamia-millia-islamia/medical-speech-recognition</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sajidala09-jamia-millia-islamia/medical-speech-recognition/runs/t72y4r4s' target=\"_blank\">https://wandb.ai/sajidala09-jamia-millia-islamia/medical-speech-recognition/runs/t72y4r4s</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/160 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daab6643a67049bb9dabd5085f879d41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d02612b7f8e14653a2f04527ec11cbad"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the final model\n# trainer.save_model(\"./whisper-medical-finetuned-final\")\n\n# Create visualizations\ndef plot_training_metrics(trainer):\n    # Get training logs\n    logs = pd.DataFrame(trainer.state.log_history)\n    \n    # Training loss plot\n    plt.figure(figsize=(12, 6))\n    plt.plot(logs[logs['loss'].notna()]['step'], logs[logs['loss'].notna()]['loss'])\n    plt.title('Training Loss Over Time')\n    plt.xlabel('Step')\n    plt.ylabel('Loss')\n    plt.savefig('training_loss.png')\n    wandb.log({\"training_loss_plot\": wandb.Image('training_loss.png')})\n    \n    # WER and CER plot\n    plt.figure(figsize=(12, 6))\n    eval_logs = logs[logs['eval_wer'].notna()]\n    plt.plot(eval_logs['step'], eval_logs['eval_wer'], label='WER')\n    plt.plot(eval_logs['step'], eval_logs['eval_cer'], label='CER')\n    plt.title('WER and CER Over Time')\n    plt.xlabel('Step')\n    plt.ylabel('Error Rate')\n    plt.legend()\n    plt.savefig('error_rates.png')\n    wandb.log({\"error_rates_plot\": wandb.Image('error_rates.png')})\n\n# Plot metrics after training\nplot_training_metrics(trainer)\n\n# Evaluate on test set\nprint(\"Evaluating model on test set...\")\neval_results = trainer.evaluate()\nprint(f\"Final evaluation results: {eval_results}\")\n\n# Function to analyze error patterns\ndef analyze_error_patterns(trainer, eval_dataset, num_samples=100):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    predictions = []\n    references = []\n    \n    for i in tqdm(range(min(num_samples, len(eval_dataset)))):\n        input_features = eval_dataset[i][\"input_features\"].unsqueeze(0).to(device)\n        with torch.no_grad():\n            pred_ids = model.generate(input_features)\n        \n        pred_text = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n        ref_text = processor.decode(eval_dataset[i][\"labels\"], skip_special_tokens=True)\n        \n        predictions.append(pred_text)\n        references.append(ref_text)\n    \n    # Calculate word-level differences\n    from difflib import SequenceMatcher\n    \n    def get_word_differences(pred, ref):\n        pred_words = pred.split()\n        ref_words = ref.split()\n        matcher = SequenceMatcher(None, pred_words, ref_words)\n        return [tag for tag in matcher.get_opcodes() if tag[0] != 'equal']\n    \n    error_types = {'substitution': 0, 'deletion': 0, 'insertion': 0}\n    \n    for pred, ref in zip(predictions, references):\n        differences = get_word_differences(pred, ref)\n        for diff in differences:\n            error_types[diff[0]] += 1\n    \n    # Plot error type distribution\n    plt.figure(figsize=(10, 6))\n    plt.bar(error_types.keys(), error_types.values())\n    plt.title('Distribution of Error Types')\n    plt.ylabel('Count')\n    plt.savefig('error_distribution.png')\n    wandb.log({\"error_distribution\": wandb.Image('error_distribution.png')})\n    \n    return error_types\n\n# Analyze error patterns\nerror_patterns = analyze_error_patterns(trainer, eval_dataset)\nprint(f\"Error pattern analysis: {error_patterns}\")\n\n# Finish wandb run\nwandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}